#
## Cirrus Tier 2 HPC Service Slurm Configuration
#
ClusterName=cirrus
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=cirrus-services1
AccountingStorageEnforce=associations,qos,limits,safe
JobAcctGatherType=jobacct_gather/linux
ControlMachine=cirrus-services1
AuthType=auth/munge

#MpiDefault=none
MpiDefault=pmi2

#Security param for CVE-2022-29500
CommunicationParameters=block_null_hash

ProctrackType=proctrack/cgroup
ReturnToService=1
SlurmUser=slurm
SwitchType=switch/none
TaskPlugin=task/affinity,task/cgroup
SlurmctldPidFile=/var/run/slurm/slurmctld.pid
#SlurmdPidFile=/var/run/slurm/slurmd.pid
SlurmdPidFile=/var/run/slurmd.pid
SlurmdSpoolDir=/var/spool/slurm
StateSaveLocation=/var/spool/slurm
#FastSchedule=1
SchedulerType=sched/backfill
#SchedulerParameters=bf_window=10080,batch_sched_delay=4,max_switch_wait=31536000
SchedulerParameters=bf_window=10080,batch_sched_delay=4,max_switch_wait=31536000,default_queue_depth=256,bf_max_job_test=1000,sched_min_interval=5000

SelectType=select/cons_tres
SelectTypeParameters=CR_CPU_Memory

# Accounting detail gathering
# N.B. Energy is only useful for exclusive jobs
JobAcctGatherFrequency=task=30,energy=30,filesystem=30
#JobAcctGatherType=jobacct_gather/cgroup
AcctGatherFilesystemType=acct_gather_filesystem/lustre
AcctGatherEnergyType=acct_gather_energy/ipmi

JobSubmitPlugins=lua
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdLogFile=/var/log/slurm/slurmd.log
MaxJobCount=1000002
MaxArraySize=1000001
KillOnBadExit=1

# 72 hours
MinJobAge=259200

# Default to not-requeuing failed jobs
JobRequeue=0

PriorityType = priority/multifactor
FairShareDampeningFactor = 1
PriorityDecayHalfLife = 4-0
PriorityCalcPeriod = 1

PriorityWeightQOS=10000
PriorityWeightAge=500
PriorityMaxAge = 28-0
PriorityWeightFairshare=300

EnforcePartLimits=ALL
PrivateData=usage

TopologyPlugin=topology/tree

GresTypes=gpu
AccountingStorageTres=gres/gpu
#DebugFlags=CPU_Bind,gres
PropagateResourceLimits=NONE
# Program ran when a node is asked to reboot itself
RebootProgram=/usr/sbin/reboot
# Number of seconds to wait for, among other things, a reboot. Shortest timed reboot was 3 minutes and 3 seconds
# Reboots take longer the more is being rebooted
ResumeTimeout=2400

# Necessary for pam_slurm_adopt to work on nodes allocated but not yet running the user's work
PrologFlags=contain,alloc,X11
Prolog=/etc/slurm/scripts/compute-prologue
Epilog=/etc/slurm/scripts/compute-epilogue
TaskProlog=/etc/slurm/scripts/compute-task-prologue
# Give messages more time on large jobs
MessageTimeout=20
# Allow 300s for Slurm jobs to finish to reduce draining nodes (SR, RFC91, 20200826)
UnkillableStepTimeout=300

# Node Healthcheck
HealthCheckProgram=/work/system/private/slurm-scripts/healthcheck.sh
HealthCheckInterval=3600
HealthCheckNodeState=ANY,CYCLE
#

# Define Nodes
# Cirrus Nodes
NodeName=r1i0n[0-35] CPUs=36 RealMemory=257400 Sockets=2 CoresPerSocket=18 ThreadsPerCore=2 State=UNKNOWN
NodeName=r1i1n[0-35] CPUs=36 RealMemory=257400 Sockets=2 CoresPerSocket=18 ThreadsPerCore=2 State=UNKNOWN
NodeName=r1i2n[0-35] CPUs=36 RealMemory=257400 Sockets=2 CoresPerSocket=18 ThreadsPerCore=2 State=UNKNOWN
NodeName=r1i3n[0-35] CPUs=36 RealMemory=257400 Sockets=2 CoresPerSocket=18 ThreadsPerCore=2 State=UNKNOWN
NodeName=r1i4n[0-35] CPUs=36 RealMemory=257400 Sockets=2 CoresPerSocket=18 ThreadsPerCore=2 State=UNKNOWN
NodeName=r1i5n[0-35] CPUs=36 RealMemory=257400 Sockets=2 CoresPerSocket=18 ThreadsPerCore=2 State=UNKNOWN
NodeName=r1i6n[0-35] CPUs=36 RealMemory=257400 Sockets=2 CoresPerSocket=18 ThreadsPerCore=2 State=UNKNOWN
NodeName=r1i7n[0-6,9-15,18-24,27-33] CPUs=36 RealMemory=257400 Sockets=2 CoresPerSocket=18 ThreadsPerCore=2 State=UNKNOWN
# EdGen Nodes
#NodeName=r2i0n[0-35],r2i1n[0-35],r2i2n[0-3,9-12,18-21,27-30] CPUs=72 RealMemory=257511 Sockets=2 CoresPerSocket=18 ThreadsPerCore=2 State=UNKNOWN
NodeName=r2i0n[0-35] CPUs=36 RealMemory=257400 Sockets=2 CoresPerSocket=18 ThreadsPerCore=2 State=UNKNOWN weight=1000
NodeName=r2i1n[0-35],r2i2n[0-3,9-12,18-21,27-30] CPUs=36 RealMemory=257400 Sockets=2 CoresPerSocket=18 ThreadsPerCore=2 State=UNKNOWN weight=1000
# GPU Nodes
NodeName=r2i3n[0-1] CPUs=40 RealMemory=385180 Sockets=2 CoresPerSocket=20 ThreadsPerCore=2 Gres=gpu:GV100GL:4 Features=skylake State=UNKNOWN

#NodeName=r2i[4-7]n[0-8] CPUs=40 RealMemory=385280 Sockets=2 CoresPerSocket=20 ThreadsPerCore=2 State=UNKNOWN
NodeName=r2i4n[0-8] CPUs=40 RealMemory=385180 Sockets=2 CoresPerSocket=20 ThreadsPerCore=2 Gres=gpu:GV100GL:4 Features=cascade State=UNKNOWN
NodeName=r2i5n[0-8] CPUs=40 RealMemory=385180 Sockets=2 CoresPerSocket=20 ThreadsPerCore=2 Gres=gpu:GV100GL:4 Features=cascade State=UNKNOWN
NodeName=r2i6n[0-8] CPUs=40 RealMemory=385180 Sockets=2 CoresPerSocket=20 ThreadsPerCore=2 Gres=gpu:GV100GL:4 Features=cascade State=UNKNOWN
NodeName=r2i7n[0-8] CPUs=40 RealMemory=385180 Sockets=2 CoresPerSocket=20 ThreadsPerCore=2 Gres=gpu:GV100GL:4 Features=cascade State=UNKNOWN

#Additional Nodes
NodeName=highmem01 CPUs=112 Boards=1 SocketsPerBoard=4 CoresPerSocket=28 ThreadsPerCore=2 RealMemory=3095568 State=UNKNOWN
# mem per cpu = 27639

# Define Basic Partitions
PartitionName=maintenance-all Nodes=ALL MaxTime=UNLIMITED DefMemPerCPU=7150 MaxMemPerCPU=7150 State=UP AllowQoS=sysadm Hidden=YES
PartitionName=maintenance-cpu Nodes=r1i0n[0-35],r1i1n[0-35],r1i2n[0-35],r1i3n[0-35],r1i4n[0-35],r1i5n[0-35],r1i6n[0-35],r1i7n[0-6,9-15,18-24,27-33],r2i0n[0-35],r2i1n[0-35],r2i2n[0-3,9-12,18-21,27-30] MaxTime=UNLIMITED DefMemPerCPU=7150 MaxMemPerCPU=7150 State=DOWN AllowQoS=sysadm,maintenance Hidden=YES
PartitionName=maintenance-gpu Nodes=r2i3n[0-1],r2i[4-7]n[0-8] MaxTime=UNLIMITED DefMemPerCPU=9628 MaxMemPerCPU=9628 State=DOWN AllowQoS=sysadm,maintenance Hidden=YES
PartitionName=maintenance-r1 Nodes=r1i0n[0-35],r1i1n[0-35],r1i2n[0-35],r1i3n[0-35],r1i4n[0-35],r1i5n[0-35],r1i6n[0-35],r1i7n[0-6,9-15,18-24,27-33] MaxTime=UNLIMITED DefMemPerCPU=7150 MaxMemPerCPU=7150 State=DOWN AllowQoS=sysadm,maintenance Hidden=YES
PartitionName=maintenance-r2 Nodes=r2i0n[0-35],r2i1n[0-35],r2i2n[0-3,9-12,18-21,27-30],r2i3n[0-1],r2i[4-7]n[0-8] MaxTime=UNLIMITED DefMemPerCPU=7150 MaxMemPerCPU=7150 State=DOWN AllowQoS=sysadm,maintenance Hidden=YES
PartitionName=maintenance-highmem Nodes=highmem01 MaxTime=UNLIMITED State=DOWN DefMemPerCPU=27639 MaxMemPerCPU=27639 AllowQoS=sysadm,maintenance Hidden=YES

# Define CPU Partitions
PartitionName=standard Nodes=r1i0n[0-35],r1i1n[0-35],r1i2n[0-35],r1i3n[0-35],r1i4n[0-35],r1i5n[0-35],r1i6n[0-35],r1i7n[0-6,9-15,18-24,27-33],r2i0n[0-35],r2i1n[0-35],r2i2n[0-3,9-12,18-21,27-30] MaxTime=UNLIMITED DefMemPerCPU=7150 MaxMemPerCPU=7150 State=UP AllowQoS=sysadm,standard,edgen,largescale,long,short,highpriority,commercial,reservation,lowpriority DefaultTime=1:00:00
PartitionName=highmem Nodes=highmem01 MaxTime=UNLIMITED DefaultTime=1:00:00 State=UP AllowQoS=sysadm,highmem,reservation DefMemPerCPU=27639 MaxMemPerCPU=27639 

# Define GPU Partitions
PartitionName=gpu Nodes=r2i3n[0-1],r2i[4-7]n[0-8] MaxTime=UNLIMITED DefMemPerCPU=9628 MaxMemPerCPU=9628 State=UP AllowQoS=gpu,short,long,largescale,reservation,lowpriority DefaultTime=1:00:00 TRESBillingWeights="GRES/gpu=1.0"

# Define Test Partitions
PartitionName=tds Nodes=r1i4n[8,17,26,35] MaxTime=UNLIMITED DefMemPerCPU=7150 MaxMemPerCPU=7150 State=UP AllowQoS=sysadm,standard AllowGroups=w01,z04 DefaultTime=1:00:00
PartitionName=gpu-tds Nodes=r2i[7]n[7-8] MaxTime=UNLIMITED DefMemPerCPU=9628 MaxMemPerCPU=9628 State=UP AllowQoS=sysadm,gpu AllowGroups=w01,z04 DefaultTime=10:00

