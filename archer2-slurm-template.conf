##
## ARCHER2 Slurm Configuration
## 
SlurmctldHost={{ slurmctld_host }}({{ slurmctld_ip }})
{% if slurmctld_backup_host %}
SlurmctldHost={{ slurmctld_backup_host }}({{ slurmctld_backup_ip }})
{% endif %}
#
#LaunchParameters=enable_nss_slurm
#MailProg=/bin/mail
MpiDefault=cray_shasta
MpiParams=ports=20000-32767
ProctrackType=proctrack/cgroup
## ARCHER2 prolog flags
PrologFlags=contain,alloc,X11
## ARCHER2 do not propagate limits from login nodes
PropagateResourceLimits=NONE
## ARCHER2 don't bring nodes immediately back into service
ReturnToService=1
SlurmctldParameters=reboot_from_controller
SlurmctldPidFile=/var/spool/slurm/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
#SlurmdUser=root
StateSaveLocation=/var/spool/slurm
SwitchType=switch/none
TaskPlugin=task/affinity,task/cgroup
KillOnBadExit=1
PlugStackConfig=/etc/slurm/plugstack.conf
Prolog=/opt/cray/atom/sbin/atom_slurm.py
Epilog=/opt/cray/atom/sbin/atom_slurm.py
# Uncomment to run ATOM tests periodically
#HealthCheckInterval=3600
#HealthCheckProgram=/opt/cray/atom/sbin/atom_slurm.py
## ARCHER2 Healthcheck
HealthCheckProgram=/work/system/private/slurm-scripts/healthcheck.sh
HealthCheckInterval=3600
HealthCheckNodeState=ANY,CYCLE
RebootProgram=/var/spool/slurm/slurm-reboot-workaround.py
ResumeTimeout=2700
#
#
# TIMERS
#KillWait=30
#MinJobAge=300
SlurmctldTimeout=30
#SlurmdTimeout=300
UnkillableStepTimeout=1800
#
#
## SCHEDULING
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory
{% if gres_types %}
GresTypes={{ gres_types }},archer2-node,compute-node
{% else %}
GresTypes=gpu,archer2-node,compute-node
{% endif %}
#
#
## LOGGING AND ACCOUNTING
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost={{ slurmdbd_ip }}
{% if slurmdbd_backup_ip %}
AccountingStorageBackupHost={{ slurmdbd_backup_ip }}
{% endif %}
AccountingStoragePort=6819
ClusterName={{ cluster_name }}
JobAcctGatherFrequency=task=30,energy=30,filesystem=30
JobAcctGatherType=jobacct_gather/linux
## ARCHER2 gather lustre stats
AcctGatherFilesystemType=acct_gather_filesystem/lustre
## ARCHER2 energy accounting override per system
#AcctGatherEnergyType=acct_gather_energy/none
{% if cluster_name == 'archer2' %}
AcctGatherEnergyType=acct_gather_energy/pm_counters
{% elif cluster_name == 'tds' %}
AcctGatherEnergyType=acct_gather_energy/ipmi
{% endif %}
## ARCHER2 accounting
AccountingStorageTres=gres/gpu,gres/archer2-node,gres/compute-node,fs/lustre
SlurmctldDebug=info
#SlurmctldLogFile=
SlurmdDebug=info
SlurmdSyslogDebug=info
#SlurmdLogFile=
#

## ARCHER2 Custom (non-overrides)
JobSubmitPlugins=lua
CommunicationParameters=block_null_hash
SlurmdParameters=l3cache_as_socket

## ARCHER2 Scheduler parameters
AccountingStorageEnforce=associations,qos,limits,safe
EnforcePartLimits=ALL
PrivateData=usage

SchedulerParameters=bf_window=5760,batch_sched_delay=4,max_switch_wait=31536000,bf_continue,defer,max_prc_cnt=150,max_rpc_cnt=1000,enable_hetjob_steps,default_queue_depth=256,sched_min_interval=2000000,bf_max_job_test=1000
MessageTimeout=30
MaxArraySize=32001
MaxJobCount=200000
JobRequeue=0
# 72 hours
MinJobAge=259200

## ARCHER2 Scheduler priority config
PriorityType = priority/multifactor
PriorityWeightQOS=10000
PriorityWeightAge=500
PriorityMaxAge = 14-0
PriorityWeightJobSize=100
PriorityDecayHalfLife = 2-0
PriorityWeightFairshare=300

## ARCHER2 Topology
TopologyPlugin=topology/tree

## ARCHER2 Partitions
PartitionName=standard Nodes=computes State=UP OverSubscribe=EXCLUSIVE MaxTime=24:00:00 DefaultTime=1:00:00 AllowQoS=sysadm,taskfarm,standard,short,long,largescale,lowpriority,highpriority,reservation,testing,weekend,profiling
PartitionName=highmem  Nodes=highmem  State=UP OverSubscribe=EXCLUSIVE MaxTime=24:00:00 DefaultTime=1:00:00 AllowQoS=sysadm,highmem,reservation
PartitionName=serial  Nodes=dvns Default=NO State=UP OverSubscribe=YES MaxTime=24:00:00 DefaultTime=1:00:00 DefMemPerCPU=1984 AllowQoS=sysadm,serial,reservation
#PartitionName=gpu Nodes=gpus Default=NO State=UP OverSubscribe=YES MaxTime=24:00:00 DefaultTime=1:00:00 AllowQoS=sysadm,gpu,reservation
#
## ARCHER2 Maintenance Partitions
PartitionName=maintenance-standard Nodes=computes State=DOWN OverSubscribe=EXCLUSIVE MaxTime=24:00:00 DefaultTime=1:00:00 AllowQoS=sysadm,taskfarm,standard,short,long,largescale,lowpriority,highpriority,reservation,testing,maintenance,weekend,profiling Hidden=YES AllowGroups=z02,z19,y01,y02
PartitionName=maintenance-standard-only Nodes=standardmem State=DOWN OverSubscribe=EXCLUSIVE MaxTime=24:00:00 DefaultTime=1:00:00 AllowQoS=sysadm,taskfarm,standard,short,long,largescale,lowpriority,highpriority,reservation,testing,maintenance,weekend,profiling Hidden=YES AllowGroups=z02,z19,y01,y02
PartitionName=maintenance-highmem Nodes=highmem State=DOWN OverSubscribe=EXCLUSIVE MaxTime=24:00:00 DefaultTime=1:00:00 AllowQoS=sysadm,highmem,reservation,maintenance Hidden=YES AllowGroups=z02,z19,y01,y02
PartitionName=maintenance-serial Nodes=dvns Default=NO State=DOWN OverSubscribe=YES MaxTime=24:00:00 DefaultTime=1:00:00 AllowQoS=sysadm,serial,reservation,maintenance Hidden=YES AllowGroups=z02,z19,y01,y02
#PartitionName=maintenance-gpu Nodes=gpus Default=NO State=DOWN OverSubscribe=YES MaxTime=24:00:00 DefaultTime=1:00:00 AllowQoS=sysadm,gpu,reservation,maintenance Hidden=YES AllowGroups=z02,z19,y01,y02
PartitionName=maintenance-all Nodes=ALL State=UP MaxTime=24:00:00 DefaultTime=1:00:00 AllowQoS=sysadm Hidden=YES AllowGroups=z02,y01
#
#
## BEGIN COMPUTE NODES
{% for node in slurm_nodes %}
NodeName={{ node.NodeName }} Sockets={{ node.Sockets }} CoresPerSocket={{ node.CoresPerSocket }} ThreadsPerCore={{ node.ThreadsPerCore }} RealMemory={{ node.RealMemory }}{{ ' Feature=COMPUTE,' + node.Feature if node.Feature }}{{ ',HighMem Weight=1000' if node.RealMemory > 262144 }}{{ ',StandardMem Weight=500' if node.RealMemory < 300000 }} Gres=archer2-node:1,compute-node:1{{ ',' + node.Gres if node.Gres }}
{% endfor %}

NodeSet=computes Feature=COMPUTE
NodeSet=standardmem Feature=StandardMem
NodeSet=highmem Feature=HighMem
## END COMPUTE NODES

## BEGIN DVN NODES
NodeName=dvn0[1-2] NodeAddr=dvn01-nmn,dvn02-nmn Sockets=2 CoresPerSocket=64 ThreadsPerCore=2 RealMemory=515240 Feature=DVN,AMD_EPYC_7742 Gres=archer2-node:1
NodeSet=dvns Feature=DVN
## END DVN NODES
#

